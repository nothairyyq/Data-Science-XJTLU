{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy.io\n",
    "import scipy.stats\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normpdf(x, mu, sigma):\n",
    "    \"\"\"\n",
    "      Computes the natural logarithm of the normal probability density function\n",
    "\n",
    "    \"\"\"\n",
    "    prob = scipy.stats.norm(mu, sigma).pdf(x)\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointParts():\n",
    "    def __init__(self, N_class):\n",
    "        self.means = np.zeros([3,N_class])\n",
    "        self.sigma = np.zeros([3,N_class])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, N_class, N_joints):\n",
    "        self.jointparts = [JointParts(N_class) for i in range(N_joints)]\n",
    "        self.class_priors = {}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(a, b):\n",
    "    \"\"\"\n",
    "       Input\n",
    "           two vectors with same size\n",
    "\n",
    "       Output\n",
    "           accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    acc = 0.0\n",
    "    for i in range(len(a)):\n",
    "        if(a[i] == b[i]):\n",
    "            acc += 1\n",
    "    acc = acc / len(a)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X):\n",
    "    \"\"\"\n",
    "      Compute the mean and variance of X, \n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO\n",
    "    mean = 0.0\n",
    "    for i in range(len(X)):\n",
    "        mean += X[i]\n",
    "    mean = mean / len(X)\n",
    "    \n",
    "    variance = 0.0\n",
    "    for i in range(len(X)):\n",
    "        tmp = X[i] - mean\n",
    "        variance += tmp * tmp.T\n",
    "    variance = variance / len(X)\n",
    "    \n",
    "\n",
    "    return (mean, variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_model(dataset, labels, G=None):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "     dataset: The data as loaded \n",
    "     labels:  The labels as loaded \n",
    "     \n",
    "    Output: the model\n",
    "     a (tentative) structure for the output model is:\n",
    "       model.class_priors: containing a vector with the prior estimations\n",
    "                           for each class\n",
    "       model.jointparts[i] contains the estimated parameters for the i-th joint\n",
    "\n",
    "            model.jointparts(i).means: a matrix of 3 x #classes with the\n",
    "                   estimated means for each of the x,y,z variables of the \n",
    "                   i-th joint and for each class.\n",
    "            model.jointparts(i).sigma: a matrix of 3 x #classes with the\n",
    "                   estimated stadar deviations for each of the x,y,z \n",
    "                   variables of the i-th joint and for each class.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO\n",
    "    ##1st Step: obtain N_samples,N_joints,N_class\n",
    "    N_samples = labels.shape[0]   #get training_data total samples\n",
    "    N_joints = dataset.shape[0]   #get training_data total joints\n",
    "    #get the total classes using bincount&count_nonzero to count\n",
    "    counts = np.bincount(labels.reshape(-1))   \n",
    "    N_class = np.count_nonzero(counts)\n",
    "\n",
    "    ##2nd Step: load Model class\n",
    "    model = Model(N_class=N_class, N_joints=N_joints)\n",
    "    \n",
    "    ##3.1 Step: compute the priori estimations\n",
    "    for i in range(len(counts)):\n",
    "        if counts[i] > 0:\n",
    "            model.class_priors[int(i)] = counts[i] / N_samples  #calculate the priori estimations\n",
    "\n",
    "    ##3.2 Step: obtain each different classes' index\n",
    "    classIndex = []\n",
    "    for i in range(len(counts)):\n",
    "        if counts[i] > 0:\n",
    "            nums = np.where(labels == i)\n",
    "            classIndex.append(nums[0])  #obtain the index of different classes\n",
    "    \n",
    "    ##3.3 Step: obtain means and standard deviations for each of the x,y,z variables of the i-th joint and for each class\n",
    "    for i in range(N_joints):\n",
    "        #3 * N_class model.jointparts[i].means\n",
    "        for j in range(N_class):\n",
    "            for a in range(3):\n",
    "                vec = dataset[i, a, classIndex[j]] #construct joint vector from each classes index\n",
    "                mean, var = fit_model(vec)\n",
    "                model.jointparts[i].means[a][j] = mean\n",
    "                model.jointparts[i].sigma[a][j] = math.sqrt(var)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_samples(instances, model):\n",
    "    \"\"\"    \n",
    "    Input\n",
    "       instance: a 20x3x#instances matrix defining body positions of\n",
    "                 instances\n",
    "       model: as the output of learn_model\n",
    "\n",
    "    Output\n",
    "       probs: a matrix of #instances x #classes with the probability of each\n",
    "              instance of belonging to each of the classes\n",
    "\n",
    "    Important: to avoid underflow numerical issues this computations should\n",
    "               be performed in log space\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO\n",
    "    ##1st Step: obtain N_samples,N_joints,N_class\n",
    "    N_samples = instances.shape[-1]\n",
    "    N_class = len(model.class_priors)\n",
    "    N_joints = instances.shape[0]\n",
    "    \n",
    "    ##construct a dict for different classes\n",
    "    indexDict = {}  #index the relevant key\n",
    "    index = 0\n",
    "    for k in model.class_priors.keys():\n",
    "        indexDict[index] = k\n",
    "        index += 1\n",
    "\n",
    "    ##construct probs non-matrix for N_samples*N_class\n",
    "    probs = np.zeros([N_samples, N_class])\n",
    "    \n",
    "    #compute the probability for each instance of each classes\n",
    "    for i in range(N_samples):\n",
    "        data = instances[:, :, i]\n",
    "        for k in range(N_class):\n",
    "            prob = 1\n",
    "            for j in range(N_joints):\n",
    "                for a in range(3):\n",
    "                    mean = model.jointparts[j].means[a][k]   # Obtain eachinstance of each classes mean and standar var\n",
    "                    sigma = model.jointparts[j].sigma[a][k]\n",
    "                    prob *= log_normpdf(data[j, a], mean, sigma)  # Computes the natural logarithm of the normal PDF\n",
    "            index = indexDict[k]\n",
    "            prob *= model.class_priors[index] #accumulate each class joint priors and likelihood to obtain prob\n",
    "            probs[i][k] = prob\n",
    "            \n",
    "#     probs = likelihood * priors\n",
    "\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST PART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = scipy.io.loadmat('data/validation_data.mat')\n",
    "X = dd['data_small']\n",
    "Y = dd['labels_small']\n",
    "# the index for training set\n",
    "ind = dd['train_indexes']\n",
    "[trainInd,nouse] = np.where(ind==1)\n",
    "# get the training set with fixed index\n",
    "X_train = X[:,:,trainInd]\n",
    "y_train = Y[trainInd]\n",
    "# training/learning\n",
    "modelNB = learn_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n"
     ]
    }
   ],
   "source": [
    "dd = scipy.io.loadmat('data/validation_data.mat')\n",
    "X = dd['data_small']\n",
    "Y = dd['labels_small']\n",
    "\n",
    "# get the test set index\n",
    "ind = dd['test_indexes']\n",
    "[testInd,nouse] = np.where(ind==1)\n",
    "# get the testing set with fixed index\n",
    "X_test = X[:,:,testInd]\n",
    "y_test = Y[testInd]\n",
    "# classify  \n",
    "y_rst = classify_samples(X_test, modelNB)\n",
    "labelpos = np.argmax(y_rst, axis = 1)\n",
    "# get the label of the each instance with probs\n",
    "classPriors = modelNB.class_priors\n",
    "whatlabel = list(classPriors.keys())\n",
    "y = classify_samples(X_test, modelNB)\n",
    "labelpos = np.argmax(y, axis = 1)\n",
    "\n",
    "rst = [whatlabel[labelpos[i]] for i in range(len(labelpos))]\n",
    "val = list(y_test.reshape(len(y_test),))\n",
    "\n",
    "print(cal_acc(rst,val))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8aca77b7e8d53afb9377d30c8cb0b3d15d2ac7a2839c3687ca79236910134408"
  },
  "kernelspec": {
   "display_name": "gempy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
